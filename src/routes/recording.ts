import {Hono} from "hono";
import {AudioFormat, ElevenLabsClient, RealtimeEvents} from "@elevenlabs/elevenlabs-js";
import {AzureOpenAI} from "openai";
import prisma from "../db";
import {authMiddleware} from "../middleware/auth";
import {uploadAudioToR2} from "../lib/r2";
import {verifyToken} from "../utils/jwt";
import {CommitStrategy} from "@elevenlabs/client";

type Variables = {
  userId: string;
};

const ELEVENLABS_API_KEY = process.env.ELEVENLABS_API_KEY;
const SAMPLE_RATE = 16000;

const AZURE_ENDPOINT = process.env.AZURE_ENDPOINT;
const AZURE_API_KEY = process.env.AZURE_API_KEY;
const AZURE_API_VERSION = process.env.AZURE_API_VERSION ?? "2023-07-01-preview";
const AZURE_DEPLOYMENT = process.env.AZURE_DEPLOYMENT_NAME ?? "gpt-4";

const elevenlabsClient = new ElevenLabsClient({ apiKey: ELEVENLABS_API_KEY });
const azureClient = new AzureOpenAI({
  apiVersion: AZURE_API_VERSION,
  endpoint: AZURE_ENDPOINT,
  apiKey: AZURE_API_KEY,
});

const CORRECTION_PROMPT = `ë„ˆëŠ” "ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ì •ê·œí™” í¸ì§‘ê¸°"ë‹¤.

ê·œì¹™(ì¤‘ìš”ë„ ìˆœ):
1) ì˜ë¯¸/ë§¥ë½ ì ˆëŒ€ ë³€ê²½ ê¸ˆì§€. ë¬¸ì¥ ì¬ì‘ì„± ìµœì†Œí™”(í•„ìš”í•œ ë¶€ë¶„ë§Œ êµì •).
2) í•œêµ­ì–´ë¡œ ì íŒ ì „ë¬¸ìš©ì–´Â·ì˜ë¬¸ë°œìŒ(ìŒì°¨)ì€ ê°€ëŠ¥í•œ í•œ ì •í™•í•œ ì›ì–´(ì˜ë¬¸, ê³µì‹ ëŒ€ì†Œë¬¸ì)ë¡œ ì¹˜í™˜. (ìµœìš°ì„ )
   ì˜ˆì‹œ: "ì—ì´í”¼ì•„ì´" â†’ "API", "ë¦¬ì•¡íŠ¸" â†’ "React", "ìë°”ìŠ¤í¬ë¦½íŠ¸" â†’ "JavaScript", 
         "ë„ì»¤" â†’ "Docker", "íƒ€ì…ìŠ¤í¬ë¦½íŠ¸" â†’ "TypeScript", "ê¹ƒí—ˆë¸Œ" â†’ "GitHub",
         "ë…¸ë“œ" â†’ "Node", "ë””ë¹„" â†’ "DB", "ìœ ì•„ì´" â†’ "UI", "ì„œë²„" â†’ "server"
3) ì˜¤íƒ€/ë§ì¶¤ë²•/ë„ì–´ì“°ê¸°/ì˜ëª» ì¸ì‹ëœ ë°œí™”ë§Œ ìì—°ìŠ¤ëŸ½ê²Œ êµì •.
4) ì½”ë“œë¸”ë¡, \`ì¸ë¼ì¸ì½”ë“œ\`, URL, íŒŒì¼ê²½ë¡œ, í‚¤/ID, ìˆ«ìÂ·ë‹¨ìœ„ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€(ëª…ë°±í•œ ì˜¤íƒ€ë§Œ ì˜ˆì™¸).

ì¶œë ¥: êµì •ëœ í…ìŠ¤íŠ¸ë§Œ. ì„¤ëª…/ì£¼ì„/ìš”ì•½ ê¸ˆì§€.`;

const SUMMARY_PROMPT = `ë„ˆëŠ” "í…ìŠ¤íŠ¸ ìš”ì•½ ì „ë¬¸ê°€"ë‹¤.

ê·œì¹™:
1) ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ìµœëŒ€ 4ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•œë‹¤.
2) ê° ë¬¸ì¥ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ë˜, ê³¼ë„í•˜ê²Œ ê¸¸ê²Œ ëŠ˜ë¦¬ì§€ ì•ŠëŠ”ë‹¤.
3) ì¤‘ìš”í•œ í‚¤ì›Œë“œì™€ ë§¥ë½ì„ ìœ ì§€í•œë‹¤.
4) ìš”ì•½ë¬¸ë§Œ ì¶œë ¥í•œë‹¤. ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ì£¼ì„ ê¸ˆì§€.

ì¶œë ¥: ìš”ì•½ëœ í…ìŠ¤íŠ¸ë§Œ (ìµœëŒ€ 4ë¬¸ì¥).`;

const TITLE_PROMPT = `ë„ˆëŠ” "ì œëª© ìƒì„± ì „ë¬¸ê°€"ë‹¤.

ê·œì¹™:
1) ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ì£¼ì œë¥¼ íŒŒì•…í•˜ì—¬ ê°„ê²°í•œ ì œëª©ì„ ìƒì„±í•œë‹¤.
2) ì œëª©ì€ ìµœëŒ€ 50ì ì´ë‚´ë¡œ ì‘ì„±í•œë‹¤.
3) êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ë˜, ì§€ë‚˜ì¹˜ê²Œ ê¸¸ì§€ ì•Šê²Œ í•œë‹¤.
4) ì œëª©ë§Œ ì¶œë ¥í•œë‹¤. ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ì£¼ì„ ê¸ˆì§€.

ì¶œë ¥: ì œëª©ë§Œ.`;

async function normalizeTextWithGpt(text: string): Promise<string> {
  try {
    console.log(`ğŸ¤– [GPT] Normalizing text: ${text.substring(0, 50)}...`);
    const response = await azureClient.chat.completions.create({
      messages: [
        { role: "system", content: CORRECTION_PROMPT },
        { role: "user", content: text },
      ],
      max_completion_tokens: 1000,
      temperature: 0.3,
      top_p: 1.0,
      model: AZURE_DEPLOYMENT,
    });
    const normalized = response.choices[0]?.message?.content?.trim() ?? text;
    console.log(`âœ… [GPT] Normalized: ${normalized}`);
    return normalized;
  } catch (e) {
    console.error("âŒ [GPT] Error:", e);
    return text;
  }
}

async function summarizeTextWithGpt(text: string): Promise<string> {
  try {
    console.log(`ğŸ¤– [GPT] Summarizing text: ${text.substring(0, 50)}...`);
    const response = await azureClient.chat.completions.create({
      messages: [
        { role: "system", content: SUMMARY_PROMPT },
        { role: "user", content: text },
      ],
      max_completion_tokens: 300,
      temperature: 0.5,
      top_p: 1.0,
      model: AZURE_DEPLOYMENT,
    });
    const summary = response.choices[0]?.message?.content?.trim() ?? '';
    console.log(`âœ… [GPT] Summary: ${summary}`);
    return summary;
  } catch (e) {
    console.error("âŒ [GPT] Summary Error:", e);
    return '';
  }
}

async function generateTitleWithGpt(text: string): Promise<string> {
  try {
    console.log(`ğŸ¤– [GPT] Generating title: ${text.substring(0, 50)}...`);
    const response = await azureClient.chat.completions.create({
      messages: [
        { role: "system", content: TITLE_PROMPT },
        { role: "user", content: text },
      ],
      max_completion_tokens: 100,
      temperature: 0.5,
      top_p: 1.0,
      model: AZURE_DEPLOYMENT,
    });
    const title = response.choices[0]?.message?.content?.trim() ?? '';
    console.log(`âœ… [GPT] Title: ${title}`);
    return title;
  } catch (e) {
    console.error("âŒ [GPT] Title Error:", e);
    return '';
  }
}

function preprocessTextWithVocabulary(text: string, pronunciationMap: Map<string, string>): string {
  if (pronunciationMap.size === 0) return text;
  
  let processed = text;
  
  // í•œê¸€ ë°œìŒì„ ì›ì–´ë¡œ ì¹˜í™˜ (ê¸´ ë‹¨ì–´ë¶€í„° ì²˜ë¦¬í•˜ì—¬ ë¶€ë¶„ ë§¤ì¹­ ë°©ì§€)
  const sortedEntries = Array.from(pronunciationMap.entries())
    .sort((a, b) => b[0].length - a[0].length);
  
  for (const [korean, original] of sortedEntries) {
    const regex = new RegExp(korean, 'gi');
    processed = processed.replace(regex, original);
  }
  
  return processed;
}

interface RecordingSession {
  sessionId: string;
  noteId: string;
  userId: string;
  audioChunks: Buffer[];
  startTime: number;
  sttConnection: any; // ElevenLabs realtime STT connection
  transcriptText: string;
  languageCode: string;
  keywordPack?: { name: string; description: string; koreanPronunciation?: string }[];
  keywordDetectionEnabled: boolean;
  externalResources?: Array<{ id: string; title: string; displayUrl: string; scrapedContent: string }>;
  resourceHintsEnabled: boolean;
  pronunciationMap: Map<string, string>; // í•œê¸€ë°œìŒ -> ì›ì–´ ë§¤í•‘
}

export const activeSessions = new Map<string, RecordingSession>();

export function createRecordingWebSocketHandler(upgradeWebSocket: any) {
  const recordingRouter = new Hono<{ Variables: Variables }>();

  // POST /session - ìƒˆ ë…¹ìŒ ì„¸ì…˜ ìƒì„±
  recordingRouter.post("/session", authMiddleware, async (c) => {
    const userId = c.get("userId");
    const { title, languageCode, keywordPackIds, externalResourceIds } = await c.req.json();

    const language = languageCode || "ko";

    console.log(`ğŸ“ [SESSION] Creating session for user: ${userId}`);
    console.log(`ğŸ“ [SESSION] Title: ${title}`);
    console.log(`ğŸ“ [SESSION] Language: ${language}`);
    console.log(`ğŸ“ [SESSION] KeywordPack IDs: ${keywordPackIds}`);
    console.log(`ğŸ“ [SESSION] ExternalResource IDs: ${externalResourceIds}`);

    const note = await prisma.note.create({
      data: {
        title: title || "Untitled Recording",
        authorId: userId,
        recordingStatus: "recording",
        durationInSeconds: 0,
        content: JSON.stringify({ languageCode: language }),
        keywordPackIds: keywordPackIds || [],
        externalResourceIds: externalResourceIds || [],
      },
    });

    console.log(`âœ… [SESSION] Session created: ${note.id}`);

    return c.json({
      sessionId: note.id,
      noteId: note.id,
      message: "Session created. Connect to WebSocket to start recording.",
    });
  });

  recordingRouter.post("/session/stop", authMiddleware, async (c) => {
    const userId = c.get("userId");
    
    console.log(`ğŸ›‘ [STOP] Stop request from user: ${userId}`);
    const { sessionId } = await c.req.json();

    if (!sessionId) {
      return c.json({ error: "sessionId is required" }, 400);
    }

    const session = activeSessions.get(sessionId);
    if (!session) {
      return c.json({ error: "Session not found or already stopped" }, 404);
    }

    if (session.userId !== userId) {
      return c.json({ error: "Access denied" }, 403);
    }

    try {
      const result = await finalizeRecording(sessionId);
      return c.json(result);
    } catch (e: any) {
      console.error(`âŒ [${sessionId}] Stop error:`, e);
      return c.json({ error: e.message || "Failed to stop recording" }, 500);
    }
  });

  recordingRouter.post("/session/cancel", authMiddleware, async (c) => {
    const userId = c.get("userId");
    const { sessionId } = await c.req.json();

    if (!sessionId) {
      return c.json({ error: "sessionId is required" }, 400);
    }

    const session = activeSessions.get(sessionId);

    const note = await prisma.note.findUnique({
      where: { id: sessionId },
    });

    if (!note) {
      return c.json({ error: "Session not found" }, 404);
    }

    if (note.authorId !== userId) {
      return c.json({ error: "Access denied" }, 403);
    }

    try {
      if (session && session.sttConnection) {
        session.sttConnection.close();
      }
      
      if (session) {
        activeSessions.delete(sessionId);
        console.log(`ğŸ—‘ï¸ [${sessionId}] Session cancelled and removed`);
      }

      // DBì—ì„œ Note ì‚­ì œ
      await prisma.note.delete({
        where: { id: sessionId },
      });

      console.log(`âœ… [${sessionId}] Recording cancelled and deleted`);

      return c.json({
        message: "Recording cancelled and deleted successfully",
        sessionId,
      });
    } catch (e: any) {
      console.error(`âŒ [${sessionId}] Cancel error:`, e);
      return c.json({ error: e.message || "Failed to cancel recording" }, 500);
    }
  });

  recordingRouter.get(
    "/session/:sessionId",
    upgradeWebSocket((c: any) => {
      const sessionId = c.req.param("sessionId");

      const authHeader = c.req.header("Authorization");
      let token = c.req.query("token");
      
      console.log(`ğŸ“¡ [${sessionId}] WebSocket upgrade request`);
      console.log(`ğŸ”‘ [${sessionId}] Auth header: ${authHeader}`);
      console.log(`ğŸ”‘ [${sessionId}] Query token: ${token}`);

      if (authHeader && authHeader.startsWith("Bearer ")) {
        token = authHeader.substring(7);
        console.log(`âœ… [${sessionId}] Token from header: ${token}`);
      } else if (token) {
        console.log(`âœ… [${sessionId}] Token from query: ${token}`);
      }

      return {
        async onOpen(_event: any, ws: any) {
          console.log(`ğŸ“¡ [${sessionId}] WebSocket connected`);

          if (!token) {
            console.log(`âŒ [${sessionId}] No token provided`);
            ws.send(JSON.stringify({ error: "Unauthorized: No token" }));
            ws.close();
            return;
          }

          const payload = verifyToken(token);
          if (!payload) {
            console.log(`âŒ [${sessionId}] Invalid token`);
            ws.send(JSON.stringify({ error: "Unauthorized: Invalid token" }));
            ws.close();
            return;
          }

          const userId = payload.userId;
          console.log(`âœ… [${sessionId}] Authenticated user: ${userId}`);

          let session: RecordingSession | undefined = activeSessions.get(sessionId);

          if (!session) {
            const note = await prisma.note.findUnique({
              where: { id: sessionId },
            });

            if (!note) {
              console.log(`âŒ [${sessionId}] Note not found`);
              ws.send(JSON.stringify({ error: "Session not found" }));
              ws.close();
              return;
            }

            if (note.authorId !== userId) {
              console.log(`âŒ [${sessionId}] User ${userId} does not own note (owner: ${note.authorId})`);
              ws.send(JSON.stringify({ error: "Access denied" }));
              ws.close();
              return;
            }

            console.log(`âœ… [${sessionId}] Access granted for user ${userId}`);

            // Load KeywordPacks if attached to the note
            let keywordPackData: { name: string; description: string; koreanPronunciation?: string }[] = [];
            const pronunciationMap = new Map<string, string>();
            
            if (note.keywordPackIds && Array.isArray(note.keywordPackIds) && note.keywordPackIds.length > 0) {
              console.log(`ğŸ“š [${sessionId}] Loading ${note.keywordPackIds.length} KeywordPacks`);
              
              const keywordPacks = await prisma.keywordPack.findMany({
                where: { id: { in: note.keywordPackIds } },
              });
              
              keywordPacks.forEach(pack => {
                if (Array.isArray(pack.keywords)) {
                  const keywords = pack.keywords as { name: string; description: string; koreanPronunciation?: string }[];
                  keywordPackData.push(...keywords);
                  
                  // Build pronunciation map for preprocessing
                  keywords.forEach(keyword => {
                    if (keyword.koreanPronunciation && keyword.koreanPronunciation.trim()) {
                      pronunciationMap.set(keyword.koreanPronunciation, keyword.name);
                    }
                  });
                }
              });
              
              console.log(`âœ… [${sessionId}] Loaded ${keywordPackData.length} total keywords from ${keywordPacks.length} packs`);
              console.log(`âœ… [${sessionId}] Built pronunciation map with ${pronunciationMap.size} entries`);
            }

            // Load ExternalResources if attached to the note
            let externalResourcesData: Array<{ id: string; title: string; displayUrl: string; scrapedContent: string }> = [];
            if (note.externalResourceIds && Array.isArray(note.externalResourceIds) && note.externalResourceIds.length > 0) {
              console.log(`ğŸ“š [${sessionId}] Loading ${note.externalResourceIds.length} ExternalResources`);
              
              const resources = await prisma.externalResource.findMany({
                where: { id: { in: note.externalResourceIds } },
                select: {
                  id: true,
                  title: true,
                  displayUrl: true,
                  scrapedContent: true,
                },
              });
              
              externalResourcesData = resources.map(r => ({
                id: r.id,
                title: r.title,
                displayUrl: r.displayUrl,
                scrapedContent: r.scrapedContent || '',
              }));
              
              console.log(`âœ… [${sessionId}] Loaded ${externalResourcesData.length} external resources`);
            }

            // Noteì˜ contentì—ì„œ languageCode ì¶”ì¶œ
            let languageCode = "ko";
            try {
              const contentData = note.content ? JSON.parse(note.content) : {};
              languageCode = contentData.languageCode || "ko";
            } catch (e) {
              console.warn(`âš ï¸ [${sessionId}] Failed to parse content, using default language: ko`);
            }

            console.log(`ğŸ™ï¸ [${sessionId}] Connecting to ElevenLabs STT...`);
            console.log(`   Model: scribe_v2_realtime`);
            console.log(`   Language: ${languageCode}`);
            console.log(`   Sample Rate: ${SAMPLE_RATE}`);
            
            const sttConnection =
              await elevenlabsClient.speechToText.realtime.connect({
                modelId: "scribe_v2_realtime",
                languageCode: languageCode,
                sampleRate: SAMPLE_RATE,
                audioFormat: AudioFormat.PCM_16000,
                commitStrategy: CommitStrategy.VAD,
                vadSilenceThresholdSecs: 1.0,
                vadThreshold: 0.3,
              });

            console.log(`âœ… [${sessionId}] ElevenLabs STT connected`);
            console.log(`   VAD Commit: enabled (1.0s silence threshold)`);

            console.log(`ğŸ” [${sessionId}] Testing STT connection...`);

            session = {
              sessionId,
              noteId: note.id,
              userId: note.authorId,
              audioChunks: [],
              startTime: Date.now(),
              sttConnection,
              transcriptText: "",
              languageCode: languageCode,
              keywordPack: keywordPackData,
              keywordDetectionEnabled: keywordPackData.length > 0,
              externalResources: externalResourcesData,
              resourceHintsEnabled: externalResourcesData.length > 0,
              pronunciationMap: pronunciationMap,
            };

              if (session) {
                  activeSessions.set(sessionId, <RecordingSession>session);
              }

            console.log(`ğŸ“¡ [${sessionId}] Setting up STT event listeners...`);
            
            sttConnection.on(
              RealtimeEvents.PARTIAL_TRANSCRIPT,
              (data: { text: string }) => {
                const text = data.text ?? "";
                
                if (text && text.trim().length > 0) {
                  console.log(`ğŸ“ [${sessionId}] PARTIAL: "${text}"`);
                  ws.send(JSON.stringify({ type: "partial", text }));
                } else {
                  console.log(`âš ï¸  [${sessionId}] Empty PARTIAL (ignoring)`);
                }
              }
            );

            sttConnection.on(
              RealtimeEvents.COMMITTED_TRANSCRIPT,
              async (data: { text: string }) => {
                const rawText = data.text ?? "";
                
                if (rawText && rawText.trim().length > 0) {
                  console.log(`âœ… [${sessionId}] COMMITTED (raw): "${rawText}"`);
                  
                  // Preprocess text with vocabulary map
                  const preprocessedText = preprocessTextWithVocabulary(rawText, session!.pronunciationMap);
                  
                  if (preprocessedText !== rawText) {
                    console.log(`ğŸ”„ [${sessionId}] PREPROCESSED: "${preprocessedText}"`);
                  }
                  
                  session!.transcriptText += preprocessedText + " ";
                  ws.send(JSON.stringify({ type: "committed", text: preprocessedText }));

                  // Check for keywords in the transcribed text
                  if (session!.keywordDetectionEnabled && session!.keywordPack && session!.keywordPack.length > 0) {
                    const detectedKeywords: { name: string; description: string }[] = [];
                    
                    session!.keywordPack.forEach(keyword => {
                      const keywordLower = keyword.name.toLowerCase();
                      const textLower = preprocessedText.toLowerCase();
                      
                      // Check if keyword appears in text (whole word match)
                      const regex = new RegExp(`\\b${keywordLower}\\b`, 'i');
                      if (regex.test(textLower)) {
                        detectedKeywords.push(keyword);
                        console.log(`ğŸ” [${sessionId}] Keyword detected: "${keyword.name}"`);
                      }
                    });

                    // Send detected keywords to client
                    if (detectedKeywords.length > 0) {
                      ws.send(JSON.stringify({ 
                        type: "keywords", 
                        keywords: detectedKeywords 
                      }));
                    }
                  }

                  // Check for hints from external resources
                  if (session!.resourceHintsEnabled && session!.externalResources && session!.externalResources.length > 0) {
                    const hints: Array<{ resourceId: string; resourceTitle: string; hint: string; sourceUrl: string }> = [];
                    
                    for (const resource of session!.externalResources) {
                      // Search for relevant content in scraped data
                      const textLower = preprocessedText.toLowerCase();
                      const contentLines = resource.scrapedContent.split('\n').filter(line => line.trim());
                      
                      // Find lines that might be relevant (simple keyword matching)
                      const words = textLower.split(/\s+/).filter(w => w.length > 2);
                      
                      for (const line of contentLines) {
                        const lineLower = line.toLowerCase();
                        let matchCount = 0;
                        
                        for (const word of words) {
                          if (lineLower.includes(word)) {
                            matchCount++;
                          }
                        }
                        
                        // If multiple words match, consider it a hint
                        if (matchCount >= 2 && line.length > 20 && line.length < 200) {
                          hints.push({
                            resourceId: resource.id,
                            resourceTitle: resource.title,
                            hint: line.trim(),
                            sourceUrl: resource.displayUrl,
                          });
                          
                          console.log(`ğŸ’¡ [${sessionId}] Hint found from "${resource.title}"`);
                          break; // Only one hint per resource per transcript
                        }
                      }
                    }

                    // Send hints to client
                    if (hints.length > 0) {
                      ws.send(JSON.stringify({ 
                        type: "hints", 
                        hints 
                      }));
                    }
                  }

                  normalizeTextWithGpt(preprocessedText).then((formattedText) => {
                    console.log(`âœ¨ [${sessionId}] FORMATTED: "${formattedText}"`);
                    ws.send(JSON.stringify({ type: "formatted", text: formattedText }));
                  }).catch((e) => {
                    console.error(`âŒ [${sessionId}] GPT formatting failed:`, e);
                  });
                } else {
                  console.log(`âš ï¸  [${sessionId}] Empty COMMITTED (ignoring)`);
                }
              }
            );

            sttConnection.on(RealtimeEvents.ERROR, (error: any) => {
              console.error(`âŒ [${sessionId}] STT ERROR:`, error);
              ws.send(JSON.stringify({ type: "error", error: String(error) }));
            });

            sttConnection.on(RealtimeEvents.OPEN, () => {
              console.log(`âœ… [${sessionId}] STT Connection READY`);
            });
            
            sttConnection.on(RealtimeEvents.CLOSE, () => {
              console.log(`ğŸ”Œ [${sessionId}] STT Connection CLOSED`);
            });

            console.log(`ğŸ“¡ [${sessionId}] Event listeners registered`);
            console.log(`ğŸ™ï¸ [${sessionId}] Recording session started (VAD: 1.0s threshold)`);
          } else {
            console.log(`ğŸ”„ [${sessionId}] Reconnected to existing session`);
          }

          ws.send(
            JSON.stringify({
              type: "ready",
              sessionId,
              message: "Ready to record",
            })
          );
        },

        async onMessage(event: any, ws: any) {
          const session = activeSessions.get(sessionId);
          if (!session) {
            ws.send(JSON.stringify({ error: "Session not found" }));
            return;
          }

          try {
            const data = JSON.parse(event.data.toString());

            // Handle keyword detection control
            if (data.action === "keyword.control") {
              if (data.data === "off") {
                session.keywordDetectionEnabled = false;
                console.log(`ğŸ”• [${sessionId}] Keyword detection disabled`);
                ws.send(JSON.stringify({ type: "keyword.status", enabled: false }));
              } else if (data.data === "on") {
                session.keywordDetectionEnabled = true;
                console.log(`ğŸ”” [${sessionId}] Keyword detection enabled`);
                ws.send(JSON.stringify({ type: "keyword.status", enabled: true }));
              }
              return;
            }

            // Handle resource hints control
            if (data.action === "hints.control") {
              if (data.data === "off") {
                session.resourceHintsEnabled = false;
                console.log(`ğŸ”• [${sessionId}] Resource hints disabled`);
                ws.send(JSON.stringify({ type: "hints.status", enabled: false }));
              } else if (data.data === "on") {
                session.resourceHintsEnabled = true;
                console.log(`ğŸ”” [${sessionId}] Resource hints enabled`);
                ws.send(JSON.stringify({ type: "hints.status", enabled: true }));
              }
              return;
            }

            if (data.audio) {
              // Base64 ì˜¤ë””ì˜¤ë¥¼ Bufferë¡œ ë³€í™˜
              const audioBuffer = Buffer.from(data.audio, "base64");
              session.audioChunks.push(audioBuffer);

              // ElevenLabsë¡œ ì „ì†¡
              if (session.sttConnection) {
                try {
                  session.sttConnection.send({
                    audioBase64: data.audio,
                  });

                  if (session.audioChunks.length % 100 === 0) {
                    console.log(`ğŸŸ¢ [${sessionId}] Sent ${session.audioChunks.length} audio chunks to ElevenLabs`);
                  }
                } catch (e) {
                  console.error(`âŒ [${sessionId}] Failed to send audio to ElevenLabs:`, e);
                }
              } else {
                console.error(`âŒ [${sessionId}] STT connection is null!`);
              }
            }
          } catch (e) {
            console.error(`âŒ [${sessionId}] Message error:`, e);
            ws.send(
              JSON.stringify({ type: "error", error: "Invalid message format" })
            );
          }
        },

        async onClose() {
          console.log(`ğŸ”Œ [${sessionId}] WebSocket disconnected`);
        },

        onError(event: any) {
          console.error(`âŒ [${sessionId}] WebSocket error:`, event);
        },
      };
    })
  );

  recordingRouter.get("/record/:noteId", authMiddleware, async (c) => {
    const userId = c.get("userId");
    const noteId = c.req.param("noteId");

    const note = await prisma.note.findUnique({
      where: { id: noteId },
    });

    if (!note) {
      return c.json({ error: "Note not found" }, 404);
    }

    if (!note.isPublic && note.authorId !== userId) {
      return c.json({ error: "Access denied" }, 403);
    }

    if (!note.recordingUrl) {
      return c.json({ error: "Recording not available" }, 404);
    }

    return c.json({
      recordingUrl: note.recordingUrl,
      durationInSeconds: note.durationInSeconds,
    });
  });

  return recordingRouter;
}


async function finalizeRecording(sessionId: string) {
  const session = activeSessions.get(sessionId);
  if (!session) {
    throw new Error("Session not found");
  }

  console.log(`ğŸ›‘ [${sessionId}] Finalizing recording...`);

  try {
    // 1. WAV íŒŒì¼ ìƒì„±
    const totalAudioBuffer = Buffer.concat(session.audioChunks);
    const durationInSeconds = Math.floor(
      (Date.now() - session.startTime) / 1000
    );

    const wavBuffer = createWavBuffer(totalAudioBuffer, SAMPLE_RATE);
    console.log(`ğŸ“ [${sessionId}] WAV file created: ${wavBuffer.length} bytes`);

    // 2. R2ì— ì—…ë¡œë“œ
    const r2Key = `recordings/${session.noteId}.wav`;
    const recordingUrl = await uploadAudioToR2(r2Key, wavBuffer, "audio/wav");
    console.log(`âœ… [${sessionId}] Uploaded to R2: ${recordingUrl}`);

    // 3. ElevenLabs Speech-to-Text API í˜¸ì¶œ (í™”ì êµ¬ë¶„ í¬í•¨)
    console.log(`ğŸ™ï¸ [${sessionId}] Calling ElevenLabs STT API...`);
    
    const languageCode = session.languageCode || 'ko';
    
    const formData = new FormData();
    // formData.append('audio', new Blob([wavBuffer], { type: 'audio/wav' }), 'recording.wav');
      formData.append("cloud_storage_url", recordingUrl);
    formData.append('model_id', 'scribe_v2');
    formData.append('language_code', languageCode);
    formData.append('diarize', 'true');

    const sttResponse = await fetch('https://api.elevenlabs.io/v1/speech-to-text', {
      method: 'POST',
      headers: {
        'xi-api-key': ELEVENLABS_API_KEY!,
      },
      body: formData,
    });

    if (!sttResponse.ok) {
      const errorText = await sttResponse.text();
      throw new Error(`ElevenLabs STT API error: ${errorText}`);
    }

    const sttResult = await sttResponse.json();
    console.log(`âœ… [${sessionId}] STT completed`);
    console.log(`   Text: ${sttResult.text?.substring(0, 100)}...`);
    console.log(`   Words: ${sttResult.words?.length || 0}`);

    // 4. GPTë¡œ ì „ì²´ í…ìŠ¤íŠ¸ êµì •
    let formattedText = sttResult.text || '';
    if (formattedText.trim()) {
      console.log(`ğŸ¤– [${sessionId}] Formatting with GPT...`);
      try {
        formattedText = await normalizeTextWithGpt(formattedText);
        console.log(`âœ… [${sessionId}] GPT formatting complete`);
      } catch (e) {
        console.error(`âš ï¸ [${sessionId}] GPT formatting failed, using original:`, e);
      }
    }

    // 5. GPTë¡œ ìš”ì•½ ìƒì„±
    let aiSummary = '';
    if (formattedText.trim()) {
      console.log(`ğŸ¤– [${sessionId}] Generating summary with GPT...`);
      try {
        aiSummary = await summarizeTextWithGpt(formattedText);
        console.log(`âœ… [${sessionId}] GPT summary complete`);
      } catch (e) {
        console.error(`âš ï¸ [${sessionId}] GPT summary failed:`, e);
      }
    }

    // 6. GPTë¡œ ì œëª© ìƒì„±
    let generatedTitle = '';
    if (formattedText.trim()) {
      console.log(`ğŸ¤– [${sessionId}] Generating title with GPT...`);
      try {
        generatedTitle = await generateTitleWithGpt(formattedText);
        console.log(`âœ… [${sessionId}] GPT title complete`);
      } catch (e) {
        console.error(`âš ï¸ [${sessionId}] GPT title generation failed:`, e);
      }
    }

    const contentJson = {
      language_code: sttResult.language_code || 'ko',
      language_probability: sttResult.language_probability || 0.0,
      text: sttResult.text || '',
      formatted_text: formattedText,
      words: sttResult.words || [],
      duration_seconds: durationInSeconds,
      sample_rate: SAMPLE_RATE,
      transcribed_at: new Date().toISOString(),
    };

    // 7. í™”ì ì •ë³´ ì¶”ì¶œ ë° ê¸°ë³¸ ì´ë¦„ ì„¤ì •
    const speakerIds = new Set<string>();
    if (sttResult.words) {
      sttResult.words.forEach((word: any) => {
        if (word.speaker_id) {
          speakerIds.add(word.speaker_id);
        }
      });
    }

    const speakers = Array.from(speakerIds)
      .sort()
      .map((speaker_id, index) => ({
        speaker_id,
        speaker_name: `ì°¸ì„ì ${index + 1}`,
      }));

    console.log(`ğŸ‘¥ [${sessionId}] Detected ${speakers.length} speakers`);

    await prisma.note.update({
      where: { id: session.noteId },
      data: {
        title: generatedTitle || undefined,
        recordingUrl,
        durationInSeconds,
        recordingStatus: "completed",
        content: JSON.stringify(contentJson, null, 2),
        aiSummary: aiSummary || null,
        speakers: speakers.length > 0 ? JSON.parse(JSON.stringify(speakers)) : null,
        lastUpdated: new Date(),
      },
    });

    console.log(`âœ… [${sessionId}] Recording finalized and saved to DB`);

    const speakerSummary: Record<string, { text: string; wordCount: number }> = {};
    if (sttResult.words) {
      sttResult.words.forEach((word: any) => {
        const speakerId = word.speaker_id || 'unknown';
        if (!speakerSummary[speakerId]) {
          speakerSummary[speakerId] = { text: '', wordCount: 0 };
        }
        speakerSummary[speakerId].text += word.text + ' ';
        speakerSummary[speakerId].wordCount++;
      });
    }

    const result = {
      message: "Recording completed and transcribed successfully",
      recordingUrl,
      durationInSeconds,
      transcript: {
        text: sttResult.text || '',
        formatted: formattedText,
        language: sttResult.language_code || 'ko',
        language_probability: sttResult.language_probability || 0.0,
        word_count: sttResult.words?.length || 0,
      },
      speakers: Object.entries(speakerSummary).map(([speakerId, data]) => ({
        speaker_id: speakerId,
        text: data.text.trim(),
        word_count: data.wordCount,
      })),
    };

    activeSessions.delete(sessionId);

    return result;
  } catch (e) {
    console.error(`âŒ [${sessionId}] Finalization error:`, e);

    await prisma.note.update({
      where: { id: session.noteId },
      data: {
        recordingStatus: "failed",
      },
    });

    throw e;
  }
}

function createWavBuffer(audioData: Buffer, sampleRate: number): Buffer {
  const numChannels = 1;
  const bitsPerSample = 16;
  const byteRate = (sampleRate * numChannels * bitsPerSample) / 8;
  const blockAlign = (numChannels * bitsPerSample) / 8;
  const dataSize = audioData.length;

  const header = Buffer.alloc(44);

  header.write("RIFF", 0);
  header.writeUInt32LE(36 + dataSize, 4);
  header.write("WAVE", 8);

  header.write("fmt ", 12);
  header.writeUInt32LE(16, 16);
  header.writeUInt16LE(1, 20);
  header.writeUInt16LE(numChannels, 22);
  header.writeUInt32LE(sampleRate, 24);
  header.writeUInt32LE(byteRate, 28);
  header.writeUInt16LE(blockAlign, 32);
  header.writeUInt16LE(bitsPerSample, 34);

  header.write("data", 36);
  header.writeUInt32LE(dataSize, 40);

  return Buffer.concat([header, audioData]);
}

export default createRecordingWebSocketHandler;
