import {Hono} from "hono";
import {AudioFormat, CommitStrategy, ElevenLabsClient, RealtimeEvents,} from "@elevenlabs/elevenlabs-js";
import {AzureOpenAI} from "openai";
import prisma from "../db";
import {authMiddleware} from "../middleware/auth";
import {uploadAudioToR2} from "../lib/r2";
import {verifyToken} from "../utils/jwt";

type Variables = {
  userId: string;
};

const ELEVENLABS_API_KEY = process.env.ELEVENLABS_API_KEY;
const SAMPLE_RATE = 16000;

const AZURE_ENDPOINT = process.env.AZURE_ENDPOINT;
const AZURE_API_KEY = process.env.AZURE_API_KEY;
const AZURE_API_VERSION = process.env.AZURE_API_VERSION ?? "2023-07-01-preview";
const AZURE_DEPLOYMENT = process.env.AZURE_DEPLOYMENT_NAME ?? "gpt-4";

const elevenlabsClient = new ElevenLabsClient({ apiKey: ELEVENLABS_API_KEY });
const azureClient = new AzureOpenAI({
  apiVersion: AZURE_API_VERSION,
  endpoint: AZURE_ENDPOINT,
  apiKey: AZURE_API_KEY,
});

const CORRECTION_PROMPT = `ë„ˆëŠ” "ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ì •ê·œí™” í¸ì§‘ê¸°"ë‹¤.

ê·œì¹™(ì¤‘ìš”ë„ ìˆœ):
1) ì˜ë¯¸/ë§¥ë½ ì ˆëŒ€ ë³€ê²½ ê¸ˆì§€. ë¬¸ì¥ ì¬ì‘ì„± ìµœì†Œí™”(í•„ìš”í•œ ë¶€ë¶„ë§Œ êµì •).
2) í•œêµ­ì–´ë¡œ ì íŒ ì „ë¬¸ìš©ì–´Â·ì˜ë¬¸ë°œìŒ(ìŒì°¨)ì€ ê°€ëŠ¥í•œ í•œ ì •í™•í•œ ì›ì–´(ì˜ë¬¸, ê³µì‹ ëŒ€ì†Œë¬¸ì)ë¡œ ì¹˜í™˜. (ìµœìš°ì„ )
3) ì˜¤íƒ€/ë§ì¶¤ë²•/ë„ì–´ì“°ê¸°/ì˜ëª» ì¸ì‹ëœ ë°œí™”ë§Œ ìì—°ìŠ¤ëŸ½ê²Œ êµì •.
4) ì½”ë“œë¸”ë¡, \`ì¸ë¼ì¸ì½”ë“œ\`, URL, íŒŒì¼ê²½ë¡œ, í‚¤/ID, ìˆ«ìÂ·ë‹¨ìœ„ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€(ëª…ë°±í•œ ì˜¤íƒ€ë§Œ ì˜ˆì™¸).

ì¶œë ¥: êµì •ëœ í…ìŠ¤íŠ¸ë§Œ. ì„¤ëª…/ì£¼ì„/ìš”ì•½ ê¸ˆì§€.`;

async function normalizeTextWithGpt(text: string): Promise<string> {
  try {
    console.log(`ğŸ¤– [GPT] Normalizing text: ${text.substring(0, 50)}...`);
    const response = await azureClient.chat.completions.create({
      messages: [
        { role: "system", content: CORRECTION_PROMPT },
        { role: "user", content: text },
      ],
      max_completion_tokens: 1000,
      temperature: 0.3,
      top_p: 1.0,
      model: AZURE_DEPLOYMENT,
    });
    const normalized = response.choices[0]?.message?.content?.trim() ?? text;
    console.log(`âœ… [GPT] Normalized: ${normalized}`);
    return normalized;
  } catch (e) {
    console.error("âŒ [GPT] Error:", e);
    return text;
  }
}

interface RecordingSession {
  sessionId: string;
  noteId: string;
  userId: string;
  audioChunks: Buffer[];
  startTime: number;
  sttConnection: any;
  transcriptText: string;
}

export const activeSessions = new Map<string, RecordingSession>();

export function createRecordingWebSocketHandler(upgradeWebSocket: any) {
  const recordingRouter = new Hono<{ Variables: Variables }>();

  // POST /session - ìƒˆ ë…¹ìŒ ì„¸ì…˜ ìƒì„±
  recordingRouter.post("/session", authMiddleware, async (c) => {
    const userId = c.get("userId");
    const { title } = await c.req.json();

    console.log(`ğŸ“ [SESSION] Creating session for user: ${userId}`);
    console.log(`ğŸ“ [SESSION] Title: ${title}`);

    const note = await prisma.note.create({
      data: {
        title: title || "Untitled Recording",
        authorId: userId,
        recordingStatus: "recording",
        durationInSeconds: 0,
      },
    });

    console.log(`âœ… [SESSION] Session created: ${note.id}`);

    return c.json({
      sessionId: note.id,
      noteId: note.id,
      message: "Session created. Connect to WebSocket to start recording.",
    });
  });

  recordingRouter.post("/session/stop", authMiddleware, async (c) => {
    const userId = c.get("userId");
    
    console.log(`ğŸ›‘ [STOP] Stop request from user: ${userId}`);
    const { sessionId } = await c.req.json();

    if (!sessionId) {
      return c.json({ error: "sessionId is required" }, 400);
    }

    const session = activeSessions.get(sessionId);
    if (!session) {
      return c.json({ error: "Session not found or already stopped" }, 404);
    }

    if (session.userId !== userId) {
      return c.json({ error: "Access denied" }, 403);
    }

    try {
      const result = await finalizeRecording(sessionId);
      return c.json(result);
    } catch (e: any) {
      console.error(`âŒ [${sessionId}] Stop error:`, e);
      return c.json({ error: e.message || "Failed to stop recording" }, 500);
    }
  });

  recordingRouter.post("/session/cancel", authMiddleware, async (c) => {
    const userId = c.get("userId");
    const { sessionId } = await c.req.json();

    if (!sessionId) {
      return c.json({ error: "sessionId is required" }, 400);
    }

    const session = activeSessions.get(sessionId);
    
    // DBì—ì„œ Note í™•ì¸
    const note = await prisma.note.findUnique({
      where: { id: sessionId },
    });

    if (!note) {
      return c.json({ error: "Session not found" }, 404);
    }

    if (note.authorId !== userId) {
      return c.json({ error: "Access denied" }, 403);
    }

    try {
      if (session) {
        if (session.sttConnection) {
          session.sttConnection.close();
        }
        activeSessions.delete(sessionId);
        console.log(`ğŸ—‘ï¸ [${sessionId}] Session cancelled and removed`);
      }

      // DBì—ì„œ Note ì‚­ì œ
      await prisma.note.delete({
        where: { id: sessionId },
      });

      console.log(`âœ… [${sessionId}] Recording cancelled and deleted`);

      return c.json({
        message: "Recording cancelled and deleted successfully",
        sessionId,
      });
    } catch (e: any) {
      console.error(`âŒ [${sessionId}] Cancel error:`, e);
      return c.json({ error: e.message || "Failed to cancel recording" }, 500);
    }
  });

  recordingRouter.get(
    "/session/:sessionId",
    upgradeWebSocket((c: any) => {
      const sessionId = c.req.param("sessionId");

      const authHeader = c.req.header("Authorization");
      let token = c.req.query("token");
      
      console.log(`ğŸ“¡ [${sessionId}] WebSocket upgrade request`);
      console.log(`ğŸ”‘ [${sessionId}] Auth header: ${authHeader}`);
      console.log(`ğŸ”‘ [${sessionId}] Query token: ${token}`);

      if (authHeader && authHeader.startsWith("Bearer ")) {
        token = authHeader.substring(7);
        console.log(`âœ… [${sessionId}] Token from header: ${token}`);
      } else if (token) {
        console.log(`âœ… [${sessionId}] Token from query: ${token}`);
      }

      return {
        async onOpen(_event: any, ws: any) {
          console.log(`ğŸ“¡ [${sessionId}] WebSocket connected`);

          if (!token) {
            console.log(`âŒ [${sessionId}] No token provided`);
            ws.send(JSON.stringify({ error: "Unauthorized: No token" }));
            ws.close();
            return;
          }

          const payload = verifyToken(token);
          if (!payload) {
            console.log(`âŒ [${sessionId}] Invalid token`);
            ws.send(JSON.stringify({ error: "Unauthorized: Invalid token" }));
            ws.close();
            return;
          }

          const userId = payload.userId;
          console.log(`âœ… [${sessionId}] Authenticated user: ${userId}`);

          let session = activeSessions.get(sessionId);

          if (!session) {
            const note = await prisma.note.findUnique({
              where: { id: sessionId },
            });

            if (!note) {
              console.log(`âŒ [${sessionId}] Note not found`);
              ws.send(JSON.stringify({ error: "Session not found" }));
              ws.close();
              return;
            }

            if (note.authorId !== userId) {
              console.log(`âŒ [${sessionId}] User ${userId} does not own note (owner: ${note.authorId})`);
              ws.send(JSON.stringify({ error: "Access denied" }));
              ws.close();
              return;
            }

            console.log(`âœ… [${sessionId}] Access granted for user ${userId}`);

            console.log(`ğŸ™ï¸ [${sessionId}] Connecting to ElevenLabs STT...`);
            console.log(`   Model: scribe_v2_realtime`);
            console.log(`   Language: ko`);
            console.log(`   Sample Rate: ${SAMPLE_RATE}`);
            
            const sttConnection =
              await elevenlabsClient.speechToText.realtime.connect({
                modelId: "scribe_v2_realtime",
                languageCode: "ko",
                sampleRate: SAMPLE_RATE,
                audioFormat: AudioFormat.PCM_16000,
                commitStrategy: CommitStrategy.VAD,
                vadSilenceThresholdSecs: 1.0,
                vadThreshold: 0.3,
              });

            console.log(`âœ… [${sessionId}] ElevenLabs STT connected`);
            console.log(`   VAD Commit: enabled (1.0s silence threshold)`);

            console.log(`ğŸ” [${sessionId}] Testing STT connection...`);

            session = {
              sessionId,
              noteId: note.id,
              userId: note.authorId,
              audioChunks: [],
              startTime: Date.now(),
              sttConnection,
              transcriptText: "",
            };

            activeSessions.set(sessionId, session);

            console.log(`ğŸ“¡ [${sessionId}] Setting up STT event listeners...`);
            
            sttConnection.on(
              RealtimeEvents.PARTIAL_TRANSCRIPT,
              (data: { text: string }) => {
                const text = data.text ?? "";
                
                if (text && text.trim().length > 0) {
                  console.log(`ğŸ“ [${sessionId}] PARTIAL: "${text}"`);
                  ws.send(JSON.stringify({ type: "partial", text }));
                } else {
                  console.log(`âš ï¸  [${sessionId}] Empty PARTIAL (ignoring)`);
                }
              }
            );

            sttConnection.on(
              RealtimeEvents.COMMITTED_TRANSCRIPT,
              async (data: { text: string }) => {
                const text = data.text ?? "";
                
                if (text && text.trim().length > 0) {
                  console.log(`âœ… [${sessionId}] COMMITTED: "${text}"`);
                  session!.transcriptText += text + " ";
                  ws.send(JSON.stringify({ type: "committed", text }));

                  normalizeTextWithGpt(text).then((formattedText) => {
                    console.log(`âœ¨ [${sessionId}] FORMATTED: "${formattedText}"`);
                    ws.send(JSON.stringify({ type: "formatted", text: formattedText }));
                  }).catch((e) => {
                    console.error(`âŒ [${sessionId}] GPT formatting failed:`, e);
                  });
                } else {
                  console.log(`âš ï¸  [${sessionId}] Empty COMMITTED (ignoring)`);
                }
              }
            );

            sttConnection.on(RealtimeEvents.ERROR, (error: any) => {
              console.error(`âŒ [${sessionId}] STT ERROR:`, error);
              ws.send(JSON.stringify({ type: "error", error: String(error) }));
            });

            sttConnection.on(RealtimeEvents.OPEN, () => {
              console.log(`âœ… [${sessionId}] STT Connection READY`);
            });
            
            sttConnection.on(RealtimeEvents.CLOSE, () => {
              console.log(`ğŸ”Œ [${sessionId}] STT Connection CLOSED`);
            });

            console.log(`ğŸ“¡ [${sessionId}] Event listeners registered`);
            console.log(`ğŸ™ï¸ [${sessionId}] Recording session started (VAD: 1.0s threshold)`);
          } else {
            console.log(`ğŸ”„ [${sessionId}] Reconnected to existing session`);
          }

          ws.send(
            JSON.stringify({
              type: "ready",
              sessionId,
              message: "Ready to record",
            })
          );
        },

        async onMessage(event: any, ws: any) {
          const session = activeSessions.get(sessionId);
          if (!session) {
            ws.send(JSON.stringify({ error: "Session not found" }));
            return;
          }

          try {
            const data = JSON.parse(event.data.toString());

            if (data.audio) {
              // Base64 ì˜¤ë””ì˜¤ë¥¼ Bufferë¡œ ë³€í™˜
              const audioBuffer = Buffer.from(data.audio, "base64");
              session.audioChunks.push(audioBuffer);

              // ElevenLabsë¡œ ì „ì†¡
              if (session.sttConnection) {
                try {
                  session.sttConnection.send({
                    audioBase64: data.audio,
                  });

                  if (session.audioChunks.length % 100 === 0) {
                    console.log(`ğŸŸ¢ [${sessionId}] Sent ${session.audioChunks.length} audio chunks to ElevenLabs`);
                  }
                } catch (e) {
                  console.error(`âŒ [${sessionId}] Failed to send audio to ElevenLabs:`, e);
                }
              } else {
                console.error(`âŒ [${sessionId}] STT connection is null!`);
              }
            }
          } catch (e) {
            console.error(`âŒ [${sessionId}] Message error:`, e);
            ws.send(
              JSON.stringify({ type: "error", error: "Invalid message format" })
            );
          }
        },

        async onClose() {
          console.log(`ğŸ”Œ [${sessionId}] WebSocket disconnected`);
        },

        onError(event: any) {
          console.error(`âŒ [${sessionId}] WebSocket error:`, event);
        },
      };
    })
  );

  recordingRouter.get("/record/:noteId", authMiddleware, async (c) => {
    const userId = c.get("userId");
    const noteId = c.req.param("noteId");

    const note = await prisma.note.findUnique({
      where: { id: noteId },
    });

    if (!note) {
      return c.json({ error: "Note not found" }, 404);
    }

    if (!note.isPublic && note.authorId !== userId) {
      return c.json({ error: "Access denied" }, 403);
    }

    if (!note.recordingUrl) {
      return c.json({ error: "Recording not available" }, 404);
    }

    return c.json({
      recordingUrl: note.recordingUrl,
      durationInSeconds: note.durationInSeconds,
    });
  });

  return recordingRouter;
}


async function finalizeRecording(sessionId: string) {
  const session = activeSessions.get(sessionId);
  if (!session) {
    throw new Error("Session not found");
  }

  console.log(`ğŸ›‘ [${sessionId}] Finalizing recording...`);

  try {
    if (session.sttConnection) {
      session.sttConnection.close();
    }

    const totalAudioBuffer = Buffer.concat(session.audioChunks);
    const durationInSeconds = Math.floor(
      (Date.now() - session.startTime) / 1000
    );

    const wavBuffer = createWavBuffer(totalAudioBuffer, SAMPLE_RATE);

    const r2Key = `recordings/${session.noteId}.wav`;
    const recordingUrl = await uploadAudioToR2(r2Key, wavBuffer, "audio/wav");

    await prisma.note.update({
      where: { id: session.noteId },
      data: {
        recordingUrl,
        durationInSeconds,
        recordingStatus: "completed",
        content: session.transcriptText.trim(),
      },
    });

    console.log(`âœ… [${sessionId}] Recording uploaded to R2: ${recordingUrl}`);

    const result = {
      message: "Recording completed and uploaded successfully",
      recordingUrl,
      durationInSeconds,
      transcriptText: session.transcriptText.trim(),
    };

    activeSessions.delete(sessionId);

    return result;
  } catch (e) {
    console.error(`âŒ [${sessionId}] Finalization error:`, e);

    await prisma.note.update({
      where: { id: session.noteId },
      data: {
        recordingStatus: "failed",
      },
    });

    throw e;
  }
}

function createWavBuffer(audioData: Buffer, sampleRate: number): Buffer {
  const numChannels = 1;
  const bitsPerSample = 16;
  const byteRate = (sampleRate * numChannels * bitsPerSample) / 8;
  const blockAlign = (numChannels * bitsPerSample) / 8;
  const dataSize = audioData.length;

  const header = Buffer.alloc(44);

  header.write("RIFF", 0);
  header.writeUInt32LE(36 + dataSize, 4);
  header.write("WAVE", 8);

  header.write("fmt ", 12);
  header.writeUInt32LE(16, 16);
  header.writeUInt16LE(1, 20);
  header.writeUInt16LE(numChannels, 22);
  header.writeUInt32LE(sampleRate, 24);
  header.writeUInt32LE(byteRate, 28);
  header.writeUInt16LE(blockAlign, 32);
  header.writeUInt16LE(bitsPerSample, 34);

  header.write("data", 36);
  header.writeUInt32LE(dataSize, 40);

  return Buffer.concat([header, audioData]);
}

export default createRecordingWebSocketHandler;
